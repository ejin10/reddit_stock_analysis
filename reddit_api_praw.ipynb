{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python366jvsc74a57bd002820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4",
   "display_name": "Python 3.6.6 64-bit ('cs229': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "02820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\timot\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# This file gets OAuth tokens\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import re\n",
    "from praw.models import MoreComments\n",
    "from tickers_names import *\n",
    "import yfinance as yf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'access_token': '-8_N7yCmIp3YayU6sFFCz7L47bVp1_A', 'token_type': 'bearer', 'expires_in': 3600, 'scope': '*'}\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE TO PUT PASSWORD AND SECRET KEY IN SEPERATE FILE BEFORE MAKING REPO PUBLIC!!!\n",
    "\n",
    "\n",
    "CLIENT_ID = \"9whZ6oWY9qQBkA\"\n",
    "SECRET_KEY = \"v4JApa9eu1WvSTmy2eaoTmRHTQ33bw\"\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "data = {'grant_type': 'client_credentials',\n",
    "        'username': 'CS229project',\n",
    "        'password': '229229'}\n",
    "headers = {'User-Agent': 'CS229project/0.0.1'}\n",
    "base_url = 'https://www.reddit.com/'\n",
    "res = requests.post(base_url + 'api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "# convert response to JSON and pull access_token value\n",
    "res_json = res.json()\n",
    "print(res_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID, \n",
    "    client_secret=SECRET_KEY, \n",
    "    user_agent='CS229project/0.0.1',\n",
    "    username='CS229project',\n",
    "    password='229229')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKEN = res_json['access_token']\n",
    "# add authorization to our headers dictionary\n",
    "headers = {'User-Agent': 'CS229project/0.0.1', 'Authorization': f'bearer {TOKEN}', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reddit_data.csv file written\n"
     ]
    }
   ],
   "source": [
    "# general post data from r/wsb hot \n",
    "r = requests.get('https://oauth.reddit.com/r/wallstreetbets/hot', headers=headers)\n",
    "data = json.loads(r.text)['data']['children']\n",
    "\n",
    "with open('reddit_data.csv', 'w', encoding='utf8') as f:  \n",
    "    writer = csv.writer(f)\n",
    "    for dp in data:\n",
    "        for key, value in dp['data'].items(): \n",
    "            writer.writerow([key, value])\n",
    "print(\"reddit_data.csv file written\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'features': {'mod_service_mute_writes': True, 'promoted_trend_blanks': True, 'show_amp_link': True, 'mweb_in_feed_refresh': {'owner': 'growth', 'variant': 'random', 'experiment_id': 507}, 'is_email_permission_required': False, 'mod_awards': True, 'mweb_xpromo_revamp_v3': {'owner': 'growth', 'variant': 'treatment_1', 'experiment_id': 480}, 'chat_subreddit': True, 'awards_on_streams': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True, 'modlog_copyright_removal': True, 'do_not_track': True, 'mod_service_mute_reads': True, 'chat_user_settings': True, 'use_pref_account_deployment': True, 'mweb_xpromo_interstitial_comments_ios': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_android': True, 'premium_subscriptions_table': True, 'mweb_xpromo_interstitial_comments_android': True, 'mweb_footer_upsell': {'owner': 'growth', 'variant': 'light_1', 'experiment_id': 497}, 'chat_group_rollout': True, 'resized_styles_images': True, 'spez_modal': True, 'noreferrer_to_noopener': True, 'swap_steps_two_and_three_recalibration': {'owner': 'growth', 'variant': 'treatment_3', 'experiment_id': 324}, 'expensive_coins_package': True}}\n"
     ]
    }
   ],
   "source": [
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "res = requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'features': {'mod_service_mute_writes': True,\n",
       "  'promoted_trend_blanks': True,\n",
       "  'show_amp_link': True,\n",
       "  'is_email_permission_required': False,\n",
       "  'mod_awards': True,\n",
       "  'expensive_coins_package': True,\n",
       "  'mweb_xpromo_revamp_v2': {'owner': 'growth',\n",
       "   'variant': 'treatment_4',\n",
       "   'experiment_id': 457},\n",
       "  'awards_on_streams': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True,\n",
       "  'chat_subreddit': True,\n",
       "  'modlog_copyright_removal': True,\n",
       "  'do_not_track': True,\n",
       "  'mod_service_mute_reads': True,\n",
       "  'chat_user_settings': True,\n",
       "  'use_pref_account_deployment': True,\n",
       "  'mweb_xpromo_interstitial_comments_ios': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_android': True,\n",
       "  'premium_subscriptions_table': True,\n",
       "  'mweb_xpromo_interstitial_comments_android': True,\n",
       "  'noreferrer_to_noopener': True,\n",
       "  'mweb_footer_upsell': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 497},\n",
       "  'chat_group_rollout': True,\n",
       "  'resized_styles_images': True,\n",
       "  'spez_modal': True,\n",
       "  'mweb_sharing_clipboard': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 315},\n",
       "  'swap_steps_two_and_three_recalibration': {'owner': 'growth',\n",
       "   'variant': 'treatment_7',\n",
       "   'experiment_id': 324}}}"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Comment' object has no attribute 'upvote_ratio'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-4dcfad83587d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mticker_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mupvote_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupvote_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[a-zA-Z]+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs229\\lib\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         raise AttributeError(\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;34mf\"{self.__class__.__name__!r} object has no attribute {attribute!r}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         )\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Comment' object has no attribute 'upvote_ratio'"
     ]
    }
   ],
   "source": [
    "#with open('all_urls.csv', 'r', encoding='utf8') as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    urls = list(url)\n",
    "urls = [\"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"]\n",
    "for url in urls:\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    comments.replace_more(limit=10)\n",
    "    i = 0\n",
    "    t_list, t_dict = convert_data()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    add_words = ('one', 'all', 'fang', 'dd', 'elon', 'time', 'long', 'call', 'ever', 'see', 'gain', 'pm', 'rent', 'omg', 'baby', 'data', 'beat', 'win', 'tiny', 'tech', 'ago', 'type', 'bod', 'eod', 'man', 'else', 'true', 'save', 'away', 'min', 'max', 'news', 'n', 'look', 'news', 'beat', 'rent', 'good', 'pre', 'rate', 'club', 'hill', 'tax', 'fds', 'sale', 'per', 'full', 'ev', 'per', 'rice', 'men', 'king', 'post', 'well', 'town', 'r', 'cdc', 'su', 'af', 'line', 'dds', 'eat', 'well', 'free', 'nice', 'cool', 'plus', 'bit', 'bro', 'sale', 'date', 'mr', 'king', 'gg', 'pay', 'per', 'fish', 'flat', 'rate', 'semi', 'game', 'im', 'kang')\n",
    "    for word in add_words:\n",
    "        stop_words.add(word)\n",
    "    # comments.replace_more(limit=0)\n",
    "    url_date = url.split('/')[-2].split('_')\n",
    "    with open('reddit_data/reddit_data_' + url_date[-3] + url_date[-2] + url_date[-1] + '.csv', 'w', encoding='utf8') as f:  \n",
    "        writer = csv.writer(f)\n",
    "        for comment in comments:\n",
    "            #print(comment.body.lower())\n",
    "            # extract mentioned stocks\n",
    "            ticker_list = []\n",
    "            words = []\n",
    "            score = comment.score\n",
    "            for word in comment.body.lower().split():\n",
    "                words.append(\" \".join(re.findall(\"[a-zA-Z]+\", word)))\n",
    "            # print(\"####\\n\", words)\n",
    "            for word in words:\n",
    "                if (word not in ticker_list) and (word not in stop_words) and (word in t_list or (len(word) > 1 and word[0] == \"$\" and word[1:] in t_list)):\n",
    "                    ticker_list.append(word)\n",
    "                elif word in t_dict and t_dict[word] not in ticker_list and word not in stop_words:\n",
    "                    ticker_list.append(t_dict[word])\n",
    "            # get percent change, write rows\n",
    "            next_day = str(dt.fromtimestamp(comment.created_utc + 86400)).split()[0]\n",
    "            seven_days_back = str(dt.fromtimestamp(comment.created_utc - 86400 * 6)).split()[0]\n",
    "            seven_days_forward = str(dt.fromtimestamp(comment.created_utc + 86400 * 8)).split()[0]\n",
    "            for ticker in ticker_list:\n",
    "                try:  \n",
    "                    df = yf.Ticker(ticker).history(start=seven_days_back, end=seven_days_forward, interval=\"1d\")\n",
    "                    percent_change_back = df.iloc[4][3] / df.iloc[0][0]\n",
    "                    percent_change_forward = df.iloc[-1][3] / df.iloc[4][0]\n",
    "                    i += 1\n",
    "                    writer.writerow([words, ticker, score, percent_change_back, percent_change_forward])\n",
    "                except:\n",
    "                    pass\n",
    "    print(\"data points written: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "urls = [\"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"]\n",
    "for url in urls:\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    print(comments[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1620988034.0\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"\n",
    "comment = reddit.submission(url=url).comments[0]\n",
    "print(comment.created_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}