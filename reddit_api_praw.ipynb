{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python366jvsc74a57bd002820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4",
   "display_name": "Python 3.6.6 64-bit ('cs229': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "02820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\timot\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# This file gets OAuth tokens\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import re\n",
    "from praw.models import MoreComments\n",
    "from tickers_names import *\n",
    "import yfinance as yf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'access_token': '-8_N7yCmIp3YayU6sFFCz7L47bVp1_A', 'token_type': 'bearer', 'expires_in': 3600, 'scope': '*'}\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE TO PUT PASSWORD AND SECRET KEY IN SEPERATE FILE BEFORE MAKING REPO PUBLIC!!!\n",
    "\n",
    "\n",
    "CLIENT_ID = \"9whZ6oWY9qQBkA\"\n",
    "SECRET_KEY = \"v4JApa9eu1WvSTmy2eaoTmRHTQ33bw\"\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "data = {'grant_type': 'client_credentials',\n",
    "        'username': 'CS229project',\n",
    "        'password': '229229'}\n",
    "headers = {'User-Agent': 'CS229project/0.0.1'}\n",
    "base_url = 'https://www.reddit.com/'\n",
    "res = requests.post(base_url + 'api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "# convert response to JSON and pull access_token value\n",
    "res_json = res.json()\n",
    "print(res_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID, \n",
    "    client_secret=SECRET_KEY, \n",
    "    user_agent='CS229project/0.0.1',\n",
    "    username='CS229project',\n",
    "    password='229229')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKEN = res_json['access_token']\n",
    "# add authorization to our headers dictionary\n",
    "headers = {'User-Agent': 'CS229project/0.0.1', 'Authorization': f'bearer {TOKEN}', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reddit_data.csv file written\n"
     ]
    }
   ],
   "source": [
    "# general post data from r/wsb hot \n",
    "r = requests.get('https://oauth.reddit.com/r/wallstreetbets/hot', headers=headers)\n",
    "data = json.loads(r.text)['data']['children']\n",
    "\n",
    "with open('reddit_data.csv', 'w', encoding='utf8') as f:  \n",
    "    writer = csv.writer(f)\n",
    "    for dp in data:\n",
    "        for key, value in dp['data'].items(): \n",
    "            writer.writerow([key, value])\n",
    "print(\"reddit_data.csv file written\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'features': {'mod_service_mute_writes': True, 'promoted_trend_blanks': True, 'show_amp_link': True, 'mweb_in_feed_refresh': {'owner': 'growth', 'variant': 'random', 'experiment_id': 507}, 'is_email_permission_required': False, 'mod_awards': True, 'mweb_xpromo_revamp_v3': {'owner': 'growth', 'variant': 'treatment_1', 'experiment_id': 480}, 'chat_subreddit': True, 'awards_on_streams': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True, 'modlog_copyright_removal': True, 'do_not_track': True, 'mod_service_mute_reads': True, 'chat_user_settings': True, 'use_pref_account_deployment': True, 'mweb_xpromo_interstitial_comments_ios': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_android': True, 'premium_subscriptions_table': True, 'mweb_xpromo_interstitial_comments_android': True, 'mweb_footer_upsell': {'owner': 'growth', 'variant': 'light_1', 'experiment_id': 497}, 'chat_group_rollout': True, 'resized_styles_images': True, 'spez_modal': True, 'noreferrer_to_noopener': True, 'swap_steps_two_and_three_recalibration': {'owner': 'growth', 'variant': 'treatment_3', 'experiment_id': 324}, 'expensive_coins_package': True}}\n"
     ]
    }
   ],
   "source": [
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "res = requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'features': {'mod_service_mute_writes': True,\n",
       "  'promoted_trend_blanks': True,\n",
       "  'show_amp_link': True,\n",
       "  'is_email_permission_required': False,\n",
       "  'mod_awards': True,\n",
       "  'expensive_coins_package': True,\n",
       "  'mweb_xpromo_revamp_v2': {'owner': 'growth',\n",
       "   'variant': 'treatment_4',\n",
       "   'experiment_id': 457},\n",
       "  'awards_on_streams': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True,\n",
       "  'chat_subreddit': True,\n",
       "  'modlog_copyright_removal': True,\n",
       "  'do_not_track': True,\n",
       "  'mod_service_mute_reads': True,\n",
       "  'chat_user_settings': True,\n",
       "  'use_pref_account_deployment': True,\n",
       "  'mweb_xpromo_interstitial_comments_ios': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_android': True,\n",
       "  'premium_subscriptions_table': True,\n",
       "  'mweb_xpromo_interstitial_comments_android': True,\n",
       "  'noreferrer_to_noopener': True,\n",
       "  'mweb_footer_upsell': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 497},\n",
       "  'chat_group_rollout': True,\n",
       "  'resized_styles_images': True,\n",
       "  'spez_modal': True,\n",
       "  'mweb_sharing_clipboard': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 315},\n",
       "  'swap_steps_two_and_three_recalibration': {'owner': 'growth',\n",
       "   'variant': 'treatment_7',\n",
       "   'experiment_id': 324}}}"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['aos', 'abt', 'abbv', 'abmd', 'acn', 'atvi', 'adbe', 'aap', 'amd', 'aes', 'afl', 'a', 'apd', 'akam', 'alk', 'alb', 'are', 'alxn', 'algn', 'alle', 'lnt', 'all', 'googl', 'goog', 'mo', 'amzn', 'amcr', 'aee', 'aal', 'aep', 'axp', 'aig', 'amt', 'awk', 'amp', 'abc', 'ame', 'amgn', 'aph', 'adi', 'anss', 'antm', 'aon', 'apa', 'aapl', 'amat', 'aptv', 'adm', 'anet', 'ajg', 'aiz', 't', 'ato', 'adsk', 'adp', 'azo', 'avb', 'avy', 'bkr', 'bll', 'bac', 'bax', 'bdx', 'brk.b', 'bby', 'bio', 'biib', 'blk', 'ba', 'bkng', 'bwa', 'bxp', 'bsx', 'bmy', 'avgo', 'br', 'bf.b', 'chrw', 'cog', 'cdns', 'czr', 'cpb', 'cof', 'cah', 'kmx', 'ccl', 'carr', 'ctlt', 'cat', 'cboe', 'cbre', 'cdw', 'ce', 'cnc', 'cnp', 'cern', 'cf', 'crl', 'schw', 'chtr']\n",
      "####\n",
      " ['me', 'a', 'pltr', 'bull', 'i', 'will', 'never', 'financially', 'recover', 'from', 'the', 'largest', 'bull', 'market', 'run', 'in', 'history']\n",
      "####\n",
      " ['i', 'wonder', 'which', 'one', 'of', 'my', 'stocks', 'elon', 'is', 'going', 'to', 'tank', 'today']\n",
      "####\n",
      " ['short', 'squeeze', 'is', 'what', 'i', 'call', 'my', 'wanking', 'sessions']\n",
      "20.395780563354492 20.236671447753906\n",
      "####\n",
      " ['i m', 'starting', 'to', 'think', 'people', 'on', 'this', 'sub', 'just', 'say', 'shit', 'and', 'have', 'no', 'fuckin', 'idea', 'what', 'they re', 'talkin', 'about']\n",
      "####\n",
      " ['my', 'accountant', 'asked', 'if', 'it', 'was', 'worth', 'it', 'to', 'do', '', 'trades', 'to', 'trail', 'the', 's p', 'by', '', 'points', 'and', 'i', 'politely', 'told', 'him', 'to', 'shut', 'his', 'whore', 'mouth']\n",
      "####\n",
      " ['everything', 'is', 'green', 'i m', 'scared']\n",
      "####\n",
      " ['you', 'see', 'that s', 'the', 'beauty', 'of', 'fds', 'you ll', 'never', 'be', 'a', 'bagholder', 'they', 'just', 'fuck', 'you', 'and', 'leave']\n",
      "56.029998779296875 55.060001373291016\n",
      "328.9143981933594 322.03146704321813\n",
      "####\n",
      " ['number', 'of', 'people', 'liquidating', 'their', 'growth', 'and', 'tech', 'portfolios', 'on', 'r stocks', 'and', 'buying', 'into', 'voo', 'and', 'vti', 'peaked', 'yesterday', '', 'bottom', 'is', 'in']\n",
      "420.1499938964844 399.7799987792969\n",
      "####\n",
      " ['i', 'cant', 'believe', 'i', 'wasted', 'so', 'many', 'years', 'on', 'the', 'internet', 'talking', 'about', 'which', 'anime', 'or', 'comics', 'characters', 'were', 'stronger', 'followed', 'by', 'how', 'i', 'wished', 'something', 'would', 'happen', 'in', 'politics', 'should', 'have', 'been', 'shit', 'posting', 'about', 'stonks', '', 'years', 'ago']\n",
      "46.7599983215332 nan\n",
      "####\n",
      " ['pltr', 'fubo', 'dkng', 'and', 'crsr', 'are', 'the', 'most', 'consistent', 'stocks', 'they', 'are', 'red', 'even', 'on', 'green', 'market', 'days', 'and', 'become', 'green', 'once', 'every', '', 'days', 'to', 'give', 'you', 'hope', 'then', 'continue', 'to', 'crash']\n",
      "####\n",
      " ['who', 'wants', 'to', 'hear', 'the', 'best', 'joke', 'ever', 'i', 'blew', 'up', 'my', 'account', 'in', 'march', '', 'second', 'half', '', 'i', 'started', 'shorting', 'the', 'markets', 'because', 'i', 'became', 'a', 'gay', 'bear', 'and', 'then', 'in', 'feb', 'this', 'year', 'i', 'capitulated', 'became', 'a', 'raging', 'bull', 'and', 'lost', 'a', 'shit', 'load', 'of', 'money', 'again', 'inverting', 'my', 'intuitions', 'is', 'probably', 'the', 'best', 'strategy', 'ever', '']\n",
      "29.8799991607666 29.989999771118164\n",
      "29.799999237060547 30.040000915527344\n",
      "####\n",
      " ['yesterday', 'was', 'my', 'first', 'time', 'getting', 'margin', 'called', 'and', 'they', 'closed', 'out', 'everything', 'oddly', 'it s', 'satisfying', 'because', 'i', 'don t', 'have', 'to', 'worry', 'about', 'something', 'that', 'isn t', 'there', 'anymore', 'lmao', 'who', 'i m', 'kidding', 'fuck', 'life']\n",
      "- TIME: No data found for this date range, symbol may be delisted\n",
      "####\n",
      " ['the', 'worst', 'thing', 'that', 'can', 'happen', 'to', 'you', 'is', 'to', 'ever', 'profit', 'from', 'a', 'gay', 'position', 'once', 'you', 'catch', 'the', 'gay', 'virus', 'you re', 'done']\n",
      "29.799999237060547 30.040000915527344\n",
      "####\n",
      " ['ok', 'so', 'this', 'entire', 'sub', 'must', 'have', 'bought', 'puts', 'yesterday', 'only', 'explanation', 'i', 'have', 'for', 'so', 'much', 'green']\n",
      "####\n",
      " ['imagine', 'the', 'value', 'of', 'the', 'dollar', 'starts', 'decreasing', 'so', 'you', 'sell', 'your', 'stock', 'in', 'order', 'to', 'get', 'more', 'of', 'those', 'dollars']\n",
      "####\n",
      " ['open', 'up', 'this', 'gat', 'damn', 'casino']\n",
      "####\n",
      " ['welcome', 'all', 'new', 'amc', 'bagholders']\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['imagine', 'thinking', 'you', 'bought', 'amc', 'but', 'you', 'bought', 'amd']\n",
      "16.40999984741211 12.59000015258789\n",
      "77.86000061035156 76.80999755859375\n",
      "####\n",
      " ['recession', 'canceled', 'back', 'to', 'wendy s']\n",
      "####\n",
      " ['if', 'wsb', 'is', 'saying', 'bull', 'trap', 'then', 'its', 'not', 'd']\n",
      "####\n",
      " ['oh', 'boy', 'it ll', 'only', 'take', 'another', 'entire', 'week', 'like', 'this', 'to', 'regain', 'my', 'losses', '']\n",
      "####\n",
      " ['daily', 'poop', 'update', 'for', 'market', 'analysis', 'thin', 'and', 'long', 'like', 'snek', 'bullish', 'indicator']\n",
      "- LONG: No data found for this date range, symbol may be delisted\n",
      "####\n",
      " ['green', 'at', 'open', 'and', 'worst', 'red', 'day', 'ever', 'by', 'lunch', 'but', 'indexes', 'see', 'a', '', 'gain', 'we', 'can', 'do', 'it', 'together', 'brothers']\n",
      "29.799999237060547 30.040000915527344\n",
      "56.029998779296875 55.060001373291016\n",
      "13.680000305175781 nan\n",
      "####\n",
      " ['if', 'amc', 'closes', 'at', '', 'i ll', 'drink', 'bleach']\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['i m', 'financially', 'unruined']\n",
      "####\n",
      " ['dude', 'this', 'pm', 'action', 'looking', 'like', 'bul', 'trap', 'sorry', 'guys', 'if', 'it', 'isn t', 'ill', 'make', 'money', 'but', 'if', 'it', 'is', 'rip']\n",
      "97.5199966430664 96.20999908447266\n",
      "####\n",
      " ['deleted']\n",
      "####\n",
      " ['i', 'really', 'have', 'made', 'every', 'wrong', 'move', 'this', 'week']\n",
      "####\n",
      " ['wife', 'almost', 'saw', 'the', 'options', 'account', 'if', 'she', 'sees', 'it', 'we', 'are', 'going', 'to', 'test', 'the', 'power', 'of', 'the', 'prenup']\n",
      "####\n",
      " ['amc', 'bagholding', 'sequence', 'complete']\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['for', 'all', 'the', 'bag', 'holders', 'about', 'to', 'die', 'we', 'appreciate', 'you']\n",
      "####\n",
      " ['', 'core', 'retail', 'sales', 'mom', '', '', '', 'previous', '', '', '', 'forecast', '', '', '', 'actual', '', 'import', 'price', 'index', 'mom', '', '', '', 'previous', '', '', '', 'forecast', '', '', '', 'actual', '', 'retail', 'sales', 'mom', '', '', '', 'previous', '', '', '', 'forecast', '', '', '', 'actual']\n",
      "45.130001068115234 45.849998474121094\n",
      "####\n",
      " ['weak', 'retail', 'sales', '', 'no', 'demand', 'to', 'drive', 'the', 'inflation', 'growth', 'stocks', 'back', 'on', 'the', 'menu']\n",
      "####\n",
      " ['priced', 'in', 'just', 'means', 'i', 'have', 'no', 'idea', 'why', 'it', 'did', 'that', 'but', 'i m', 'going', 'to', 'pretend', 'anyways']\n",
      "####\n",
      " ['press', 'f', 'for', 'amc', 'fomotards', 'at', '']\n",
      "12.8100004196167 12.520000457763672\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['got', 'my', 'upcoming', 'vote', 'reminder', 'for', 'bb', 'and', 'i ll', 'be', 'voting', 'for', 'them', 'to', 'raise', 'their', 'stock', 'price']\n",
      "####\n",
      " ['mfs', 'with', '', 'shares', 'in', 'amc', 'saying', 'don t', 'sell', '']\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['amc', 'is', '', 'today', 'or', 'back', 'to', '']\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['futures', 'fake', 'no', 'way', 'wall', 'street', 'let s', 'me', 'recover', 'any', 'losses', 'before', 'closing', 'my', 'position', 'dump', 'at', 'open']\n",
      "####\n",
      " ['if', 'pltr', 'is', 'over', '', 'eod', 'i', 'will', 'get', 'hammered', 'drunk', 'tonight', 'if', 'pltr', 'is', 'under', '', 'eod', 'i', 'will', 'get', 'hammered', 'drunk', 'tonight', 'i', 'promise', 'to', 'honor', 'this']\n",
      "5.610000133514404 5.539999961853027\n",
      "####\n",
      " ['i', 'have', '', 'stocks', 'on', 'the', 'top', 'loser', 'board', 'on', 'fidelity', '']\n",
      "####\n",
      " ['man', 'i', 'love', 'a', 'winning', 'a', 'trade', 'would', 'love', 'to', 'be', 'a', 'part', 'of', 'one', 'someday']\n",
      "119.79349517822266 118.69483500056928\n",
      "####\n",
      " ['anyone', 'else', 'love', 'waking', 'up', 'in', 'the', 'morning', 'and', 'seeing', 'futures', 'just', 'ripping']\n",
      "4.510000228881836 4.5\n",
      "####\n",
      " ['bilbag', 'holdbaggins']\n",
      "####\n",
      " ['any', 'time', 'a', 'stock', 'goes', 'up', 'by', '', '', '', 'short', 'squeeze', 'any', 'time', 'a', 'stock', 'goes', 'down', 'by', '', '', 'theyre', 'shorting', 'the', 'shit', 'out', 'of', 'it']\n",
      "- TIME: No data found for this date range, symbol may be delisted\n",
      "####\n",
      " ['glad', 'i', 'sold', 'my', 'amc', 'calls', 'this', 'morning']\n",
      "10.949999809265137 nan\n",
      "16.40999984741211 12.59000015258789\n",
      "####\n",
      " ['beautiful', 'close', 'to', 'a', 'fucking', 'horrible', 'week']\n",
      "####\n",
      " ['just', 'need', '', 'or', '', 'more', 'of', 'these', 'days', 'to', 'not', 'be', 'poor']\n",
      "####\n",
      " ['well', 'the', 'ape', 'spam', 'has', 'started', 'see', 'you', 'guys', 'this', 'evening', 'when', 'they', 'are', 'all', 'tucked', 'in', 'for', 'bed', 'good', 'luck', 'out', 'there', 'mods']\n",
      "74.83000183105469 72.54000091552734\n",
      "56.029998779296875 55.060001373291016\n",
      "20.6200008392334 nan\n",
      "####\n",
      " ['amc', 'holders', 'congrats', 'you', 'pushed', 'the', 'price', 'to', '', 'but', 'now', 'you', 'need', 'new', 'bagholders', 'to', 'sell', 'your', 'shares', 'good', 'luck', 'with', 'that']\n",
      "16.40999984741211 12.59000015258789\n",
      "20.6200008392334 nan\n",
      "num comments:  50\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"\n",
    "comments = reddit.submission(url=url).comments[:50]\n",
    "i = 0\n",
    "t, t_dict = convert_data()\n",
    "t_list = [x.lower() for x in t[1:]]\n",
    "print(t_list[0:100])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "add_words = ('one', 'all', 'fang', 'dd', 'elon')\n",
    "for word in add_words:\n",
    "    stop_words.add(word)\n",
    "# comments.replace_more(limit=0)\n",
    "with open('reddit_data/reddit_data.csv', 'w', encoding='utf8') as f:  \n",
    "    writer = csv.writer(f)\n",
    "    for comment in comments:\n",
    "        #print(comment.body.lower())\n",
    "        i += 1\n",
    "        # extract mentioned stocks\n",
    "        ticker_list = []\n",
    "        words = []\n",
    "        for word in comment.body.lower().split():\n",
    "            words.append(\" \".join(re.findall(\"[a-zA-Z]+\", word)))\n",
    "        print(\"####\\n\", words)\n",
    "        for word in words:\n",
    "            if (word in t_list or (len(word) > 1 and word[0] == \"$\" and word[1:] in t_list)) and (word not in ticker_list) and (word not in stop_words):\n",
    "                ticker_list.append(word)\n",
    "            elif word in t_dict and t_dict[word] not in ticker_list and word not in stop_words:\n",
    "                ticker_list.append(t_dict[word])\n",
    "        # get percent change, write rows\n",
    "        next_day = str(dt.fromtimestamp(comment.created_utc + 86400)).split()[0]\n",
    "        for ticker in ticker_list:\n",
    "            try:  \n",
    "                df = yf.Ticker(ticker).history(start=next_day, interval=\"5d\")\n",
    "                print(df.iloc[1][3], df.iloc[0][0])\n",
    "                percent_change = df.iloc[1][3] / df.iloc[0][0]\n",
    "                writer.writerow([comment.body.lower(), ticker, percent_change])\n",
    "            except:\n",
    "                pass\n",
    "print(\"num comments: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1620988034.0\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"\n",
    "comment = reddit.submission(url=url).comments[0]\n",
    "print(comment.created_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}