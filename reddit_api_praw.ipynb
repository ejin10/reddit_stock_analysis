{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python366jvsc74a57bd002820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4",
   "display_name": "Python 3.6.6 64-bit ('cs229': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "02820e647ccb6fa35f8f30dcdc1d9afda8f3817f309a398a968b8883ccf348a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\timot\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# This file gets OAuth tokens\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import praw\n",
    "import re\n",
    "from praw.models import MoreComments\n",
    "from tickers_names import *\n",
    "import yfinance as yf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'access_token': '-xtwAu2KI_1WrJjF6-XTstE6JFjEuXA', 'token_type': 'bearer', 'expires_in': 3600, 'scope': '*'}\n"
     ]
    }
   ],
   "source": [
    "# MAKE SURE TO PUT PASSWORD AND SECRET KEY IN SEPERATE FILE BEFORE MAKING REPO PUBLIC!!!\n",
    "\n",
    "\n",
    "CLIENT_ID = \"9whZ6oWY9qQBkA\"\n",
    "SECRET_KEY = \"v4JApa9eu1WvSTmy2eaoTmRHTQ33bw\"\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "data = {'grant_type': 'client_credentials',\n",
    "        'username': 'CS229project',\n",
    "        'password': '229229'}\n",
    "headers = {'User-Agent': 'CS229project/0.0.1'}\n",
    "base_url = 'https://www.reddit.com/'\n",
    "res = requests.post(base_url + 'api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "# convert response to JSON and pull access_token value\n",
    "res_json = res.json()\n",
    "print(res_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID, \n",
    "    client_secret=SECRET_KEY, \n",
    "    user_agent='CS229project/0.0.1',\n",
    "    username='CS229project',\n",
    "    password='229229')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOKEN = res_json['access_token']\n",
    "# add authorization to our headers dictionary\n",
    "headers = {'User-Agent': 'CS229project/0.0.1', 'Authorization': f'bearer {TOKEN}', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reddit_data.csv file written\n"
     ]
    }
   ],
   "source": [
    "# general post data from r/wsb hot \n",
    "r = requests.get('https://oauth.reddit.com/r/wallstreetbets/hot', headers=headers)\n",
    "data = json.loads(r.text)['data']['children']\n",
    "\n",
    "with open('reddit_data.csv', 'w', encoding='utf8') as f:  \n",
    "    writer = csv.writer(f)\n",
    "    for dp in data:\n",
    "        for key, value in dp['data'].items(): \n",
    "            writer.writerow([key, value])\n",
    "print(\"reddit_data.csv file written\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'features': {'mod_service_mute_writes': True, 'promoted_trend_blanks': True, 'show_amp_link': True, 'mweb_in_feed_refresh': {'owner': 'growth', 'variant': 'random', 'experiment_id': 507}, 'is_email_permission_required': False, 'mod_awards': True, 'mweb_xpromo_revamp_v3': {'owner': 'growth', 'variant': 'treatment_1', 'experiment_id': 480}, 'chat_subreddit': True, 'awards_on_streams': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True, 'modlog_copyright_removal': True, 'do_not_track': True, 'mod_service_mute_reads': True, 'chat_user_settings': True, 'use_pref_account_deployment': True, 'mweb_xpromo_interstitial_comments_ios': True, 'mweb_xpromo_modal_listing_click_daily_dismissible_android': True, 'premium_subscriptions_table': True, 'mweb_xpromo_interstitial_comments_android': True, 'mweb_footer_upsell': {'owner': 'growth', 'variant': 'light_1', 'experiment_id': 497}, 'chat_group_rollout': True, 'resized_styles_images': True, 'spez_modal': True, 'noreferrer_to_noopener': True, 'swap_steps_two_and_three_recalibration': {'owner': 'growth', 'variant': 'treatment_3', 'experiment_id': 324}, 'expensive_coins_package': True}}\n"
     ]
    }
   ],
   "source": [
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "res = requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'features': {'mod_service_mute_writes': True,\n",
       "  'promoted_trend_blanks': True,\n",
       "  'show_amp_link': True,\n",
       "  'is_email_permission_required': False,\n",
       "  'mod_awards': True,\n",
       "  'expensive_coins_package': True,\n",
       "  'mweb_xpromo_revamp_v2': {'owner': 'growth',\n",
       "   'variant': 'treatment_4',\n",
       "   'experiment_id': 457},\n",
       "  'awards_on_streams': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True,\n",
       "  'chat_subreddit': True,\n",
       "  'modlog_copyright_removal': True,\n",
       "  'do_not_track': True,\n",
       "  'mod_service_mute_reads': True,\n",
       "  'chat_user_settings': True,\n",
       "  'use_pref_account_deployment': True,\n",
       "  'mweb_xpromo_interstitial_comments_ios': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_android': True,\n",
       "  'premium_subscriptions_table': True,\n",
       "  'mweb_xpromo_interstitial_comments_android': True,\n",
       "  'noreferrer_to_noopener': True,\n",
       "  'mweb_footer_upsell': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 497},\n",
       "  'chat_group_rollout': True,\n",
       "  'resized_styles_images': True,\n",
       "  'spez_modal': True,\n",
       "  'mweb_sharing_clipboard': {'owner': 'growth',\n",
       "   'variant': 'control_1',\n",
       "   'experiment_id': 315},\n",
       "  'swap_steps_two_and_three_recalibration': {'owner': 'growth',\n",
       "   'variant': 'treatment_7',\n",
       "   'experiment_id': 324}}}"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.csv', 'r', encoding='utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    urls = []\n",
    "    for row in reader:\n",
    "        urls.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- MIL: No data found for this date range, symbol may be delisted\n",
      "- ATE: No data found for this date range, symbol may be delisted\n",
      "- MUH: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- P: No data found for this date range, symbol may be delisted\n",
      "- OWW: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- LO: No data found for this date range, symbol may be delisted\n",
      "- MEET: No data found, symbol may be delisted\n",
      "- GOV: No data found for this date range, symbol may be delisted\n",
      "- ISH: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- BOI: No data found for this date range, symbol may be delisted\n",
      "- JOY: No data found for this date range, symbol may be delisted\n",
      "- P: No data found for this date range, symbol may be delisted\n",
      "- SPAN: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- Q: No data found for this date range, symbol may be delisted\n",
      "- P: No data found for this date range, symbol may be delisted\n",
      "- P: No data found for this date range, symbol may be delisted\n",
      "- MEET: No data found, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- HK: No data found, symbol may be delisted\n",
      "- ATE: No data found for this date range, symbol may be delisted\n",
      "- JOY: No data found for this date range, symbol may be delisted\n",
      "- MIL: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "- BORN: No data found, symbol may be delisted\n",
      "- LEVY: No data found, symbol may be delisted\n",
      "- BORN: No data found, symbol may be delisted\n",
      "- SYNC: No data found for this date range, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "-  RKT: No data found, symbol may be delisted\n",
      "may132021done,  data points written:  1000\n",
      "- INB: No data found, symbol may be delisted\n",
      "- VR: No data found for this date range, symbol may be delisted\n",
      "- PE: No data found, symbol may be delisted\n",
      "- BORN: No data found, symbol may be delisted\n",
      "- MEET: No data found, symbol may be delisted\n",
      "- BID: No data found, symbol may be delisted\n",
      "- ROSE: No data found, symbol may be delisted\n",
      "- Q: No data found for this date range, symbol may be delisted\n",
      "- SPAN: No data found for this date range, symbol may be delisted\n",
      "- BORN: No data found, symbol may be delisted\n",
      "- BORN: No data found, symbol may be delisted\n",
      "- FTD: No data found, symbol may be delisted\n",
      "- CART: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "#urls = [\"https://www.reddit.com/r/wallstreetbets/comments/nm414o/daily_discussion_thread_for_may_27_2021/\"]\n",
    "for url in urls[2:70]:\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    comments.replace_more(limit=32)\n",
    "    i = 0\n",
    "    t_list, t_dict = convert_data()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    add_words = ('one', 'all', 'fang', 'dd', 'elon', 'time', 'long', 'call', 'ever', 'see', 'gain', 'pm', 'rent', 'omg', 'baby', 'data', 'beat', 'win', 'tiny', 'tech', 'ago', 'type', 'bod', 'eod', 'man', 'else', 'true', 'save', 'away', 'min', 'max', 'news', 'n', 'look', 'news', 'beat', 'rent', 'good', 'pre', 'rate', 'club', 'hill', 'tax', 'fds', 'sale', 'per', 'full', 'ev', 'per', 'rice', 'men', 'king', 'post', 'well', 'town', 'r', 'cdc', 'su', 'af', 'line', 'dds', 'eat', 'well', 'free', 'nice', 'cool', 'plus', 'bit', 'bro', 'sale', 'date', 'mr', 'king', 'gg', 'pay', 'per', 'fish', 'flat', 'rate', 'semi', 'game', 'im', 'kang', 'act', 'via', 'avg', 'jfc', 'sir', 'lock', 'core', 'hear', 'fun', 'fast', 'big', 'rock', 'cash', 'air', 'gold', 'hr', 'car', 'trip', 'door', 'ape', 'hot', 'code', 'sons', 'fuel', 'mini', 'name', 'play', 'calm', 'two', 'jobs', 'glad', 'cost', 'tho', 'mo', 'luv', 'ship', 'fly', 'ppl', 'mind', 'glad', 'zen', 'mark', 'blue', 'leaf', 'name', 'low', 'wood', 'jazz', 'cuz', 'k', 'stay', 'wash', 'earn', 'ed', 'ego', 'farm', 'flex', 'fold', 'form', 'fuel', 'soul', 'star', 'stay', 'tap', 'good', 'hear', 'heat', 'hero', 'herb', 'hi', 'hive', 'hog', 'huge', 'ice', 'idea', 'if', 'it', 'jack', 'job', 'jobs', 'joy', 'keg', 'key', 'keys', 'lad', 'leg', 'lime', 'lock', 'look', 'ma', 'mark', 'meet', 'meme', 'mil', 'mill', 'more', 'move', 'my', 'nice', 'nine', 'oink', 'one', 'peg', 'plow', 'plus', 'pope', 'pro', 'pool', 'punk', 'pure', 'push', 'rare', 'rate', 'rent', 'roll', 'rick', 'salt', 'sand', 'see', 'semi', 'ship', 'sky', 'slot', 'snow', 'soap', 'soda', 'sons', 'tier', 'tiny', 'tier', 'tops', 'town', 'tree', 'vest', 'walk', 'wash', 'warm', 'wave', 'wild', 'wrap') \n",
    "    for word in add_words:\n",
    "        stop_words.add(word)\n",
    "    # comments.replace_more(limit=0)\n",
    "    url_date = url.split('/')[-2].split('_')\n",
    "    with open('reddit_data/reddit_data_' + url_date[-3] + url_date[-2] + url_date[-1] + '.csv', 'w', encoding='utf8') as f:  \n",
    "        writer = csv.writer(f)\n",
    "        for comment in comments:\n",
    "            #print(comment.body.lower())\n",
    "            # extract mentioned stocks\n",
    "            ticker_list = []\n",
    "            words = []\n",
    "            score = comment.score\n",
    "            for word in comment.body.lower().split():\n",
    "                words.append(\" \".join(re.findall(\"[a-zA-Z]+\", word)))\n",
    "            # print(\"####\\n\", words)\n",
    "            for word in words:\n",
    "                if (word not in ticker_list) and (word not in stop_words) and (word in t_list or (len(word) > 1 and word[0] == \"$\" and word[1:] in t_list)):\n",
    "                    ticker_list.append(word)\n",
    "                elif word in t_dict and t_dict[word] not in ticker_list and word not in stop_words:\n",
    "                    ticker_list.append(t_dict[word])\n",
    "            # get percent change, write rows\n",
    "            next_day = str(dt.fromtimestamp(comment.created_utc + 86400)).split()[0]\n",
    "            seven_days_back = str(dt.fromtimestamp(comment.created_utc - 86400 * 6)).split()[0]\n",
    "            seven_days_forward = str(dt.fromtimestamp(comment.created_utc + 86400 * 8)).split()[0]\n",
    "            for ticker in ticker_list:\n",
    "                try:  \n",
    "                    df = yf.Ticker(ticker).history(start=seven_days_back, end=seven_days_forward, interval=\"1d\")\n",
    "                    percent_change_back = df.iloc[4][3] / df.iloc[0][0]\n",
    "                    percent_change_forward = df.iloc[-1][3] / df.iloc[4][0]\n",
    "                    i += 1\n",
    "                    writer.writerow([words, ticker, score, percent_change_back, percent_change_forward])\n",
    "                except:\n",
    "                    pass\n",
    "            if i >= 1000:\n",
    "                break\n",
    "    print(url_date[-3] + url_date[-2] + url_date[-1] + \"done, \", \"data points written: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "urls = [\"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"]\n",
    "for url in urls:\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    print(comments[0].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1620988034.0\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/wallstreetbets/comments/nc4z12/daily_discussion_thread_for_may_14_2021/\"\n",
    "comment = reddit.submission(url=url).comments[0]\n",
    "print(comment.created_utc)"
   ]
  }
 ]
}