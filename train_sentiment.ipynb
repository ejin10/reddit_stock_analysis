{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data \n",
    "import torchtext\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce GTX 1080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 145027\n",
      "Number of valid data: 18128\n",
      "Number of test data: 163155\n"
     ]
    }
   ],
   "source": [
    "# Declare fields for tweets and labels\n",
    "# include_lengths tells the RNN how long the actual sequences are\n",
    "# spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenize, lower=True, include_lengths=True)\n",
    "UPVOTE = data.LabelField(sequential=False, use_vocab=False, dtype=torch.int64)\n",
    "CHANGE = data.LabelField(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "SENT_LABEL = data.LabelField(sequential=False, dtype=torch.int64)\n",
    "CHAN_LABEL = data.LabelField(sequential=False, dtype=torch.int64)\n",
    "\n",
    "# Map data to fields\n",
    "fields = [('text', TEXT), ('upvote', None), ('change', None), ('sent_label', SENT_LABEL), ('chan_label', None)]\n",
    "\n",
    "# Apply field definition to create torch dataset\n",
    "train_data = torchtext.legacy.data.TabularDataset(\n",
    "        path=\"train_data.csv\",\n",
    "        format=\"CSV\",\n",
    "        fields=fields,\n",
    "        skip_header=False)\n",
    "\n",
    "valid_data = torchtext.legacy.data.TabularDataset(\n",
    "        path=\"valid_data.csv\",\n",
    "        format=\"CSV\",\n",
    "        fields=fields,\n",
    "        skip_header=False)\n",
    "\n",
    "test_data = torchtext.legacy.data.TabularDataset(\n",
    "        path=\"test_data.csv\",\n",
    "        format=\"CSV\",\n",
    "        fields=fields,\n",
    "        skip_header=False)\n",
    "\n",
    "print(\"Number of train data: {}\".format(len(train_data)))\n",
    "print(\"Number of valid data: {}\".format(len(valid_data)))\n",
    "print(\"Number of test data: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Build vocab and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 145027),\n",
       " ('gme', 23666),\n",
       " ('spy', 19902),\n",
       " ('call', 18742),\n",
       " ('aapl', 14643),\n",
       " ('nio', 13227),\n",
       " ('tesla', 11658),\n",
       " ('today', 10423),\n",
       " ('buy', 10418),\n",
       " ('go', 9792)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 10000000\n",
    "\n",
    "# unk_init initializes words in the vocab using the Gaussian distribution\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "# build vocab for training set - convert words into integers\n",
    "SENT_LABEL.build_vocab(train_data)\n",
    "\n",
    "# Most frequent tokens\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# sort_within_batch sorts all the tensors within a batch by their lengths\n",
    "(train_iterator, valid_iterator, test_iterator) = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    device = device,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rshuai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rshuai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/rshuai/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(29210, 100, padding_idx=1)\n",
      "  (encoder): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (predictor): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.sentiment_model import SentimentLSTM\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "# 2 layers of biLSTM\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "# Dropout probability\n",
    "DROPOUT = 0.5\n",
    "# Get pad token index from vocab\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Create an instance of LSTM class\n",
    "model = SentimentLSTM(INPUT_DIM,\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            OUTPUT_DIM,\n",
    "            N_LAYERS,\n",
    "            BIDIRECTIONAL,\n",
    "            DROPOUT,\n",
    "            PAD_IDX)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29210, 100])\n"
     ]
    }
   ],
   "source": [
    "# Copy the pre-trained word embeddings into the embedding layer\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "# [vocab size, embedding dim]\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5816, -0.1002,  0.6567,  ..., -1.0982, -2.0876, -0.4422],\n",
       "        [-0.0334,  1.9419, -0.1282,  ..., -1.0294, -0.5595, -0.6538],\n",
       "        [ 0.1734, -0.5768,  0.8373,  ...,  0.6636,  0.2077,  0.0641],\n",
       "        ...,\n",
       "        [-0.4172, -0.4581,  1.2460,  ...,  0.1602, -0.0563,  1.3825],\n",
       "        [ 0.4593,  0.0251,  0.0319,  ..., -0.0136, -0.5006,  0.5798],\n",
       "        [-0.0310,  0.5543, -0.1443,  ..., -1.2080,  1.9179, -0.2627]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the initial weights of the embedding layer with the pre-trained embeddings\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1734, -0.5768,  0.8373,  ...,  0.6636,  0.2077,  0.0641],\n",
      "        ...,\n",
      "        [-0.4172, -0.4581,  1.2460,  ...,  0.1602, -0.0563,  1.3825],\n",
      "        [ 0.4593,  0.0251,  0.0319,  ..., -0.0136, -0.5006,  0.5798],\n",
      "        [-0.0310,  0.5543, -0.1443,  ..., -1.2080,  1.9179, -0.2627]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize <unk> and <pad> both to all zeros - irrelevant for sentiment analysis\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "# Setting row in the embedding weights matrix to zero using the token index\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer used to update the weights\n",
    "# optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "# Loss function: binary cross entropy with logits\n",
    "# It restricts the predictions to a number between 0 and 1 using the logit function\n",
    "# then use the bound scarlar to calculate the loss using binary cross entropy\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use GPU\n",
    "model = model.to(device)\n",
    "# criterion = criterion.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(predictions, label):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch.\n",
    "\n",
    "    predictions - float\n",
    "    label - 0 or 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Round predictions to the closest integer using the sigmoid function\n",
    "    preds = torch.round(torch.sigmoid(predictions))\n",
    "    # If prediction is equal to label\n",
    "    correct = (preds == label).float()\n",
    "    # Average correct predictions\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def timer(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Returns the minutes and seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    time = end_time - start_time\n",
    "    mins = int(time / 60)\n",
    "    secs = int(time - (mins * 60))\n",
    "\n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to evaluate training loss and accuracy.\n",
    "\n",
    "    iterator - train iterator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cumulated Training loss\n",
    "    training_loss = 0.0\n",
    "    # Cumulated Training accuracy\n",
    "    training_acc = 0.0\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # For each batch in the training iterator\n",
    "    for batch in iterator:\n",
    "        \n",
    "        # 1. Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # batch.text is a tuple (tensor, len of seq)\n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        # 2. Compute the predictions\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = criterion(predictions, batch.sent_label.float())\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = batch_accuracy(predictions, batch.sent_label.float())\n",
    "        \n",
    "        # 4. Use loss to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        training_acc += accuracy.item()\n",
    "        \n",
    "    # Return the loss and accuracy, averaged across each epoch\n",
    "    # len of iterator = num of batches in the iterator\n",
    "    return training_loss / len(iterator), training_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Function to evaluate the loss and accuracy of validation and test sets.\n",
    "\n",
    "    iterator - validation or test iterator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cumulated Training loss\n",
    "    eval_loss = 0.0\n",
    "    # Cumulated Training accuracy\n",
    "    eval_acc = 0\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Don't calculate the gradients\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.sent_label.float())\n",
    "            \n",
    "            accuracy = batch_accuracy(predictions, batch.sent_label.float())\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            eval_acc += accuracy.item()\n",
    "        \n",
    "    return eval_loss / len(iterator), eval_acc / len(iterator)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is 0.001\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.31 | Train Accuracy: 86.72%\n",
      "\t Validation Loss 0.19 | Validation Accuracy: 91.22%\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.15 | Train Accuracy: 94.15%\n",
      "\t Validation Loss 0.12 | Validation Accuracy: 95.52%\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.11 | Train Accuracy: 95.93%\n",
      "\t Validation Loss 0.07 | Validation Accuracy: 97.46%\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.08 | Train Accuracy: 97.01%\n",
      "\t Validation Loss 0.06 | Validation Accuracy: 97.97%\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.06 | Train Accuracy: 97.63%\n",
      "\t Validation Loss 0.06 | Validation Accuracy: 97.8%\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.05 | Train Accuracy: 98.04%\n",
      "\t Validation Loss 0.05 | Validation Accuracy: 98.27%\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.05 | Train Accuracy: 98.28%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 97.3%\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.04 | Train Accuracy: 98.52%\n",
      "\t Validation Loss 0.05 | Validation Accuracy: 98.2%\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.03 | Train Accuracy: 98.8%\n",
      "\t Validation Loss 0.05 | Validation Accuracy: 98.54%\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.03 | Train Accuracy: 98.87%\n",
      "\t Validation Loss 0.05 | Validation Accuracy: 98.57%\n",
      "learning rate is 0.005\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.05 | Train Accuracy: 98.23%\n",
      "\t Validation Loss 0.06 | Validation Accuracy: 98.07%\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.04 | Train Accuracy: 98.6%\n",
      "\t Validation Loss 0.07 | Validation Accuracy: 98.1%\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.03 | Train Accuracy: 98.84%\n",
      "\t Validation Loss 0.07 | Validation Accuracy: 98.2%\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.03 | Train Accuracy: 98.98%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 98.13%\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.03 | Train Accuracy: 99.04%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 97.94%\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.03 | Train Accuracy: 99.09%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 98.14%\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.02 | Train Accuracy: 99.13%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 98.24%\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.02 | Train Accuracy: 99.15%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 98.29%\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.02 | Train Accuracy: 99.15%\n",
      "\t Validation Loss 0.08 | Validation Accuracy: 98.16%\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.02 | Train Accuracy: 99.14%\n",
      "\t Validation Loss 0.09 | Validation Accuracy: 98.17%\n",
      "learning rate is 0.01\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss 0.04 | Train Accuracy: 98.6%\n",
      "\t Validation Loss 0.09 | Validation Accuracy: 97.8%\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss 0.09 | Train Accuracy: 96.64%\n",
      "\t Validation Loss 0.09 | Validation Accuracy: 97.02%\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 17.71%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "learning rate is 0.05\n",
      "Epoch 1:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 2:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 3:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 4:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 5:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 6:\n",
      "\t Total Time: 0m 13s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 7:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 8:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 9:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n",
      "Epoch 10:\n",
      "\t Total Time: 0m 12s\n",
      "\t Train Loss nan | Train Accuracy: 0.0%\n",
      "\t Validation Loss nan | Validation Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Lowest validation lost\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "learning_rates = [1e-3, 5e-3, 1e-2, 5e-2]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'learning rate is {lr}')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Evaluate training loss and accuracy\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        # Evaluate validation loss and accuracy\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        mins, secs = timer(start_time, end_time)\n",
    "\n",
    "        # At each epoch, if the validation loss is the best\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # Save the parameters of the model\n",
    "            torch.save(model.state_dict(), 'trained_sentiment.pt')\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch+1))\n",
    "        print(\"\\t Total Time: {}m {}s\".format(mins, secs))\n",
    "        print(\"\\t Train Loss {} | Train Accuracy: {}%\".format(round(train_loss, 2), round(train_acc*100, 2)))\n",
    "        print(\"\\t Validation Loss {} | Validation Accuracy: {}%\".format(round(valid_loss, 2), round(valid_acc*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.02 | Test Acc: 99.5%\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the best validation loss\n",
    "model.load_state_dict(torch.load('trained_sentiment.pt'))\n",
    "\n",
    "# Evaluate test loss and accuracy\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(\"Test Loss: {} | Test Acc: {}%\".format(round(test_loss, 2), round(test_acc*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squad",
   "language": "python",
   "name": "squad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
